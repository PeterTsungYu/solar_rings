{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "1. An introduction to ConvLSTM\n",
    "    https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7\n",
    "2. How to work with Time Distributed data in a neural network\n",
    "    https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00\n",
    "3. A Visual Guide to Recurrent Layers in Keras\n",
    "    https://amitness.com/2020/04/recurrent-layers-keras/\n",
    "4. coursera-functional api\n",
    "    https://www.coursera.org/lecture/customising-models-tensorflow2/multiple-inputs-and-outputs-XVZYB\n",
    "5. keras - functional api\n",
    "    https://www.tensorflow.org/guide/keras/functional\n",
    "6. Advanced Keras â€” Constructing Complex Custom Losses and Metrics\n",
    "    https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n",
    "7. tensorflow: save and load model\n",
    "    https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "8. tensorflow: training and evaluation\n",
    "    https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "9. Write custom callback\n",
    "    https://www.tensorflow.org/guide/keras/custom_callback/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Model import TimeCNN, ConvLSTM, factor, input_window, predict_window\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import math, square\n",
    "from tensorflow.keras import Input, models, optimizers, Model, metrics\n",
    "from tensorflow.keras.backend import function\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Concatenate, Multiply\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback\n",
    "#from tensorflow.keras.layers import Conv2D, ConvLSTM2D, BatchNormalization, MaxPooling3D, TimeDistributed, Flatten, Dense, Concatenate, Multiply, Add \n",
    "#from tensorflow import concat, split, math, square, constant\n",
    "#from tensorflow.keras.backend import squeeze, reshape\n",
    "\n",
    "#force channels-first ordering\n",
    "from keras import backend\n",
    "backend.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define size of input ports and output ports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 days for 1 days prediction\n",
    "\n",
    "# data size params\n",
    "file_len = len(glob.glob(\"./FeatMap/AM/*.npy\"))\n",
    "timesteps = 3 # use 3 hr to predict 1 future hour (3 for 1)\n",
    "future = 1 #*24\n",
    "stride = 1\n",
    "batch_size = None\n",
    "\n",
    "input_ports = {'atm':(2, 200, 155), # channels: 'Pressure-Corrected AirMass', 'Cosine Incidence Angle' \n",
    "               'rain':(2, 200, 155), # channels: 'RH', 'Precp'\n",
    "               'wind':(3, 200, 155), # channels: 'WS', 'WD_cos', 'WD_sin'\n",
    "               'cloud':(1, 200, 155) # channels: 'StaImg'\n",
    "               #'air':(200, 155, 2) # channels: air pollution\n",
    "              }\n",
    "\n",
    "target = {'etr':(1, 200, 155), \n",
    "          'hour':(1, 200, 155)\n",
    "         } # elements: 'ETR from SOLPOS', '1h-unit matrix'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input ports and output ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = np.load(\"./etr_2017.npy\")\n",
    "sample_mask = (np.sum(np.sum(np.sum(etr, axis=-1), -1), -1) != 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atm = np.load(\"./atm_2017.npy\")[sample_mask]\n",
    "rain = np.load(\"./rain_2017.npy\")[sample_mask]\n",
    "wind = np.load(\"./wind_2017.npy\")[sample_mask]\n",
    "cloud = np.load(\"./cloud_2017.npy\")[sample_mask]\n",
    "irr = np.load(\"./irr_2017.npy\")[sample_mask]\n",
    "hour = np.load(\"./hour_2017.npy\")[sample_mask]\n",
    "shine = np.load(\"./shine_2017.npy\")[sample_mask]\n",
    "etr = etr[sample_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert atm.shape[0] == rain.shape[0]\n",
    "assert wind.shape[0] == rain.shape[0]\n",
    "assert cloud.shape[0] == rain.shape[0]\n",
    "assert irr.shape[0] == rain.shape[0]\n",
    "assert hour.shape[0] == rain.shape[0]\n",
    "assert shine.shape[0] == rain.shape[0]\n",
    "assert etr.shape[0] == rain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = atm.shape[0]\n",
    "# array indexing (index as array)\n",
    "ind = np.random.permutation(data_len)\n",
    "num_val_samples = int(data_len*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "## (inputs, targets, sample_weights)\n",
    "# You can either pass a flat (1D) Numpy array with the same length as the input samples \n",
    "## (1:1 mapping between weights and samples), \n",
    "### or in the case of temporal data, you can pass a 2D array with shape \n",
    "### (samples, sequence_length), to apply a different weight to every timestep of every sample.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"atm\": atm[ind][:-num_val_samples], \n",
    "         \"rain\": rain[ind][:-num_val_samples], \n",
    "         \"wind\": wind[ind][:-num_val_samples], \n",
    "         \"cloud\": cloud[ind][:-num_val_samples], \n",
    "         \"hour_space\": hour[ind][:-num_val_samples], \n",
    "         \"etr_space\": etr[ind][:-num_val_samples]\n",
    "        },\n",
    "        {\"hour_ground_pred\": shine[ind][:-num_val_samples], \n",
    "         \"irr_ground_pred\": irr[ind][:-num_val_samples]\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"atm\": atm[ind][-num_val_samples:], \n",
    "         \"rain\": rain[ind][-num_val_samples:], \n",
    "         \"wind\": wind[ind][-num_val_samples:], \n",
    "         \"cloud\": cloud[ind][-num_val_samples:], \n",
    "         \"hour_space\": hour[ind][-num_val_samples:], \n",
    "         \"etr_space\": etr[ind][-num_val_samples:]\n",
    "        },\n",
    "        {\"hour_ground_pred\": shine[ind][-num_val_samples:], \n",
    "         \"irr_ground_pred\": irr[ind][-num_val_samples:]\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_compiled_model(batch_size=None, device=None):\n",
    "\n",
    "    # build model\n",
    "    # create input ports \n",
    "    space_inputs = [Input(name=f'{key}_space', shape=value, batch_size=batch_size) for key,value in target.items()]\n",
    "    port_inputs = [Input(name=f'{key}', shape=(timesteps,) + value, batch_size=batch_size) for key,value in input_ports.items()]\n",
    "\n",
    "    # TimeDistributed CNN layers\n",
    "    port_FeatMaps = [TimeCNN(i) for i in port_inputs]\n",
    "    #print(inputs)\n",
    "    #print(port_FeatMaps)\n",
    "\n",
    "    # concat layer to stack four feature ports \n",
    "    port_concat = Concatenate(axis=2, name='port_concat')(port_FeatMaps) # concat axis: channel\n",
    "\n",
    "    # ConvLSTM layers\n",
    "    coef_FeatMap = ConvLSTM(inputs=port_concat)\n",
    "\n",
    "    # two branches for two output ports\n",
    "    pred_Etr, pred_Hour = [factor(inputs=coef_FeatMap, raw=space_inputs[i], port_len=len(input_ports)) for i in range(len(target))]\n",
    "\n",
    "    # unit conversion from ETR[W/m2] to Irr[MJ/m2]\n",
    "    ## Irr = ETR*(60*60*Hour)*10^-6\n",
    "    pred_time = math.scalar_mul(3600/1000000, pred_Hour, name='unit_conversion')\n",
    "    pred_Irr = Multiply(name='irr_ground_pred')([pred_Etr, pred_time])\n",
    "\n",
    "    # connect functional api\n",
    "    model = Model(inputs=[port_inputs, space_inputs], outputs=[pred_Irr, pred_Hour])\n",
    "\n",
    "    #model.summary()\n",
    "    plot_model(model, \"Model.png\", show_shapes=True)\n",
    "\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.RMSprop(1e-3),\n",
    "        loss={\n",
    "            \"irr_ground_pred\": MapLoss(loss_size=int(batch_size/device)),\n",
    "            \"hour_ground_pred\": MapLoss(loss_size=int(batch_size/device))\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"irr_ground_pred\": 1.0,\n",
    "            \"hour_ground_pred\": 0.5 # since the hour ground truth accuracy is not high enough\n",
    "        },\n",
    "        metrics={\n",
    "            \"irr_ground_pred\": [\n",
    "                metrics.MeanAbsoluteError()\n",
    "                #metrics.MeanAbsolutePercentageError()\n",
    "            ],\n",
    "            \"hour_ground_pred\": [metrics.MeanAbsoluteError()]\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapLoss(loss_size):\n",
    "    bool_mask = np.load('./TruthMap/BoolMask.npy') \n",
    "    bool_mask = np.repeat(np.expand_dims(np.expand_dims(bool_mask, 0), 0), loss_size, 0)\n",
    "    #print(bool_mask.shape)\n",
    "    \n",
    "    def Loss(y_true, y_pred):\n",
    "    # computes the mean squared error between the real data and the prediction    \n",
    "        return math.reduce_mean(square(y_true[bool_mask] - y_pred[bool_mask]))    \n",
    "    \n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 # 24 hr prediction as a batch\n",
    "\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "device = strategy.num_replicas_in_sync\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model(batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 48 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "500/500 [==============================] - 156s 312ms/step - loss: 0.2777 - irr_ground_pred_loss: 0.2300 - hour_ground_pred_loss: 0.0955 - irr_ground_pred_mean_absolute_error: 0.5336 - hour_ground_pred_mean_absolute_error: 0.2065 - val_loss: 0.0000e+00 - val_irr_ground_pred_loss: 0.0000e+00 - val_hour_ground_pred_loss: 0.0000e+00 - val_irr_ground_pred_mean_absolute_error: 0.0000e+00 - val_hour_ground_pred_mean_absolute_error: 0.0000e+00\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 85s 170ms/step - loss: 0.2839 - irr_ground_pred_loss: 0.2359 - hour_ground_pred_loss: 0.0955 - irr_ground_pred_mean_absolute_error: 0.5363 - hour_ground_pred_mean_absolute_error: 0.2080 - val_loss: 0.2877 - val_irr_ground_pred_loss: 0.2394 - val_hour_ground_pred_loss: 0.0967 - val_irr_ground_pred_mean_absolute_error: 0.5153 - val_hour_ground_pred_mean_absolute_error: 0.2044\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 93s 185ms/step - loss: 0.2768 - irr_ground_pred_loss: 0.2290 - hour_ground_pred_loss: 0.0953 - irr_ground_pred_mean_absolute_error: 0.5358 - hour_ground_pred_mean_absolute_error: 0.2078 - val_loss: 0.2772 - val_irr_ground_pred_loss: 0.2282 - val_hour_ground_pred_loss: 0.0980 - val_irr_ground_pred_mean_absolute_error: 0.5629 - val_hour_ground_pred_mean_absolute_error: 0.2215\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 86s 172ms/step - loss: 0.2771 - irr_ground_pred_loss: 0.2294 - hour_ground_pred_loss: 0.0955 - irr_ground_pred_mean_absolute_error: 0.5345 - hour_ground_pred_mean_absolute_error: 0.2071 - val_loss: 0.3221 - val_irr_ground_pred_loss: 0.2688 - val_hour_ground_pred_loss: 0.1067 - val_irr_ground_pred_mean_absolute_error: 0.4584 - val_hour_ground_pred_mean_absolute_error: 0.1735\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 86s 172ms/step - loss: 0.2920 - irr_ground_pred_loss: 0.2426 - hour_ground_pred_loss: 0.0987 - irr_ground_pred_mean_absolute_error: 0.5328 - hour_ground_pred_mean_absolute_error: 0.2043 - val_loss: 0.2723 - val_irr_ground_pred_loss: 0.2221 - val_hour_ground_pred_loss: 0.1004 - val_irr_ground_pred_mean_absolute_error: 0.5187 - val_hour_ground_pred_mean_absolute_error: 0.2036\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 87s 174ms/step - loss: 0.2714 - irr_ground_pred_loss: 0.2246 - hour_ground_pred_loss: 0.0939 - irr_ground_pred_mean_absolute_error: 0.5346 - hour_ground_pred_mean_absolute_error: 0.2071 - val_loss: 0.2806 - val_irr_ground_pred_loss: 0.2329 - val_hour_ground_pred_loss: 0.0953 - val_irr_ground_pred_mean_absolute_error: 0.5252 - val_hour_ground_pred_mean_absolute_error: 0.2156\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 87s 175ms/step - loss: 0.2749 - irr_ground_pred_loss: 0.2274 - hour_ground_pred_loss: 0.0947 - irr_ground_pred_mean_absolute_error: 0.5322 - hour_ground_pred_mean_absolute_error: 0.2077 - val_loss: 0.2871 - val_irr_ground_pred_loss: 0.2372 - val_hour_ground_pred_loss: 0.0998 - val_irr_ground_pred_mean_absolute_error: 0.5153 - val_hour_ground_pred_mean_absolute_error: 0.2010\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 85s 169ms/step - loss: 0.2719 - irr_ground_pred_loss: 0.2247 - hour_ground_pred_loss: 0.0948 - irr_ground_pred_mean_absolute_error: 0.5320 - hour_ground_pred_mean_absolute_error: 0.2051 - val_loss: 0.2706 - val_irr_ground_pred_loss: 0.2208 - val_hour_ground_pred_loss: 0.0997 - val_irr_ground_pred_mean_absolute_error: 0.5201 - val_hour_ground_pred_mean_absolute_error: 0.2045\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 95s 189ms/step - loss: 0.2690 - irr_ground_pred_loss: 0.2221 - hour_ground_pred_loss: 0.0934 - irr_ground_pred_mean_absolute_error: 0.5336 - hour_ground_pred_mean_absolute_error: 0.2070 - val_loss: 0.2768 - val_irr_ground_pred_loss: 0.2292 - val_hour_ground_pred_loss: 0.0951 - val_irr_ground_pred_mean_absolute_error: 0.5198 - val_hour_ground_pred_mean_absolute_error: 0.2056\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 85s 170ms/step - loss: 0.2720 - irr_ground_pred_loss: 0.2249 - hour_ground_pred_loss: 0.0938 - irr_ground_pred_mean_absolute_error: 0.5312 - hour_ground_pred_mean_absolute_error: 0.2049 - val_loss: 0.2718 - val_irr_ground_pred_loss: 0.2249 - val_hour_ground_pred_loss: 0.0938 - val_irr_ground_pred_mean_absolute_error: 0.5335 - val_hour_ground_pred_mean_absolute_error: 0.2101\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 87s 173ms/step - loss: 0.2691 - irr_ground_pred_loss: 0.2222 - hour_ground_pred_loss: 0.0938 - irr_ground_pred_mean_absolute_error: 0.5317 - hour_ground_pred_mean_absolute_error: 0.2058 - val_loss: 0.2790 - val_irr_ground_pred_loss: 0.2313 - val_hour_ground_pred_loss: 0.0954 - val_irr_ground_pred_mean_absolute_error: 0.5031 - val_hour_ground_pred_mean_absolute_error: 0.2031\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 88s 175ms/step - loss: 0.2723 - irr_ground_pred_loss: 0.2259 - hour_ground_pred_loss: 0.0924 - irr_ground_pred_mean_absolute_error: 0.5318 - hour_ground_pred_mean_absolute_error: 0.2065 - val_loss: 0.2758 - val_irr_ground_pred_loss: 0.2294 - val_hour_ground_pred_loss: 0.0928 - val_irr_ground_pred_mean_absolute_error: 0.5108 - val_hour_ground_pred_mean_absolute_error: 0.2021\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 88s 175ms/step - loss: 0.2612 - irr_ground_pred_loss: 0.2159 - hour_ground_pred_loss: 0.0907 - irr_ground_pred_mean_absolute_error: 0.5298 - hour_ground_pred_mean_absolute_error: 0.2059 - val_loss: 0.2632 - val_irr_ground_pred_loss: 0.2147 - val_hour_ground_pred_loss: 0.0971 - val_irr_ground_pred_mean_absolute_error: 0.5266 - val_hour_ground_pred_mean_absolute_error: 0.2094\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 88s 175ms/step - loss: 0.2645 - irr_ground_pred_loss: 0.2184 - hour_ground_pred_loss: 0.0915 - irr_ground_pred_mean_absolute_error: 0.5283 - hour_ground_pred_mean_absolute_error: 0.2052 - val_loss: 0.2553 - val_irr_ground_pred_loss: 0.2094 - val_hour_ground_pred_loss: 0.0919 - val_irr_ground_pred_mean_absolute_error: 0.5425 - val_hour_ground_pred_mean_absolute_error: 0.2198\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 87s 173ms/step - loss: 0.2633 - irr_ground_pred_loss: 0.2176 - hour_ground_pred_loss: 0.0913 - irr_ground_pred_mean_absolute_error: 0.5316 - hour_ground_pred_mean_absolute_error: 0.2081 - val_loss: 0.2600 - val_irr_ground_pred_loss: 0.2132 - val_hour_ground_pred_loss: 0.0937 - val_irr_ground_pred_mean_absolute_error: 0.5306 - val_hour_ground_pred_mean_absolute_error: 0.2090\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 94s 189ms/step - loss: 0.2645 - irr_ground_pred_loss: 0.2185 - hour_ground_pred_loss: 0.0915 - irr_ground_pred_mean_absolute_error: 0.5305 - hour_ground_pred_mean_absolute_error: 0.2066 - val_loss: 0.2570 - val_irr_ground_pred_loss: 0.2107 - val_hour_ground_pred_loss: 0.0927 - val_irr_ground_pred_mean_absolute_error: 0.5281 - val_hour_ground_pred_mean_absolute_error: 0.2067\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 85s 171ms/step - loss: 0.2660 - irr_ground_pred_loss: 0.2199 - hour_ground_pred_loss: 0.0918 - irr_ground_pred_mean_absolute_error: 0.5304 - hour_ground_pred_mean_absolute_error: 0.2062 - val_loss: 0.2637 - val_irr_ground_pred_loss: 0.2177 - val_hour_ground_pred_loss: 0.0920 - val_irr_ground_pred_mean_absolute_error: 0.5223 - val_hour_ground_pred_mean_absolute_error: 0.2073\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 87s 175ms/step - loss: 0.2621 - irr_ground_pred_loss: 0.2168 - hour_ground_pred_loss: 0.0908 - irr_ground_pred_mean_absolute_error: 0.5318 - hour_ground_pred_mean_absolute_error: 0.2076 - val_loss: 0.2601 - val_irr_ground_pred_loss: 0.2140 - val_hour_ground_pred_loss: 0.0922 - val_irr_ground_pred_mean_absolute_error: 0.5433 - val_hour_ground_pred_mean_absolute_error: 0.2115\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 91s 181ms/step - loss: 0.2619 - irr_ground_pred_loss: 0.2161 - hour_ground_pred_loss: 0.0912 - irr_ground_pred_mean_absolute_error: 0.5297 - hour_ground_pred_mean_absolute_error: 0.2066 - val_loss: 0.2790 - val_irr_ground_pred_loss: 0.2306 - val_hour_ground_pred_loss: 0.0970 - val_irr_ground_pred_mean_absolute_error: 0.5014 - val_hour_ground_pred_mean_absolute_error: 0.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "500/500 [==============================] - 88s 175ms/step - loss: 0.2627 - irr_ground_pred_loss: 0.2174 - hour_ground_pred_loss: 0.0906 - irr_ground_pred_mean_absolute_error: 0.5303 - hour_ground_pred_mean_absolute_error: 0.2086 - val_loss: 0.2737 - val_irr_ground_pred_loss: 0.2254 - val_hour_ground_pred_loss: 0.0966 - val_irr_ground_pred_mean_absolute_error: 0.5682 - val_hour_ground_pred_mean_absolute_error: 0.2253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe2d33a940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 20\n",
    "\n",
    "# Prepare a directory to store all the checkpoints.\n",
    "now = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H:%M:%S')\n",
    "checkpoint_dir = f\"./Model_mcp/{now}\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "mcp = ModelCheckpoint(filepath=checkpoint_dir + \"/{epoch:02d}_val_loss={val_loss:.4f}.h5\", \n",
    "                      #save_freq = 30,\n",
    "                      save_best_only=True,\n",
    "                      save_weights_only=True, # for model.load_weights\n",
    "                      monitor='val_loss',\n",
    "                      mode='auto',\n",
    "                      verbose=0\n",
    "                     )\n",
    "\n",
    "# tensorboard\n",
    "viz = TensorBoard(\n",
    "    log_dir=\"./Tensorboard\",\n",
    "    histogram_freq=1,  # How often to log histogram visualizations\n",
    "    embeddings_freq=1,  # How often to log embedding visualizations\n",
    "    update_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_dataset.shuffle(\n",
    "        buffer_size=(data_len-int(data_len*0.1)), \n",
    "        reshuffle_each_iteration=True\n",
    "    ).batch(batch_size),\n",
    "    epochs=epoch,\n",
    "    validation_data=val_dataset.shuffle(\n",
    "        buffer_size=int(data_len*0.1), \n",
    "        reshuffle_each_iteration=True\n",
    "    ).batch(batch_size),\n",
    "    #x={\"atm\": atm, \"rain\": rain, \"wind\": wind, \"cloud\": cloud, \"hour_space\": hour, \"etr_space\": etr},\n",
    "    #y={\"hour_ground_pred\": shine, \"irr_ground_pred\":irr},\n",
    "    #validation_data=(atm, rain, wind, cloud, hour, etr, shine, irr),\n",
    "    #validation_batch_size,\n",
    "    #sample_weight=[sample_weight,sample_weight],\n",
    "    #batch_size=batch_size,\n",
    "    #steps_per_epoch=sample_size/batch_size,\n",
    "    #validation_split=0.1,\n",
    "    #shuffle=True, # shuffle every epoch\n",
    "    callbacks=[mcp,\n",
    "               #viz\n",
    "              ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
